{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978eb653",
   "metadata": {},
   "source": [
    "# Script 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pysnmp.hlapi import *\n",
    "import schedule\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to collect SNMP data (SNMPv3)\n",
    "def get_snmp_data_v3(target, oid, user, auth_key, priv_key, port=161):\n",
    "    snmp_engine = SnmpEngine()\n",
    "    usm_user_data = UsmUserData(\n",
    "        user,\n",
    "        authKey=auth_key,\n",
    "        privKey=priv_key,\n",
    "        authProtocol=usmHMACSHAAuthProtocol,  # Authentication Protocol\n",
    "        privProtocol=usmAesCfb128Protocol     # Privacy Protocol\n",
    "    )\n",
    "    transport = UdpTransportTarget((target, port))  # UDP/IPv4 Transport\n",
    "    context = ContextData()\n",
    "    object_type = ObjectType(ObjectIdentity(oid))\n",
    "\n",
    "    iterator = getCmd(\n",
    "        snmp_engine,\n",
    "        usm_user_data,\n",
    "        transport,\n",
    "        context,\n",
    "        object_type\n",
    "    )\n",
    "\n",
    "    errorIndication, errorStatus, errorIndex, varBinds = next(iterator)\n",
    "\n",
    "    if errorIndication:\n",
    "        print(f\"Error: {errorIndication}\")\n",
    "        return None\n",
    "    elif errorStatus:\n",
    "        print(f\"Error: {errorStatus.prettyPrint()}\")\n",
    "        return None\n",
    "    else:\n",
    "        for varBind in varBinds:\n",
    "            return varBind[1].prettyPrint()\n",
    "\n",
    "# Router IP\n",
    "router = '192.168.40.1'\n",
    "\n",
    "# SNMPv3 Credentials (example)\n",
    "user = 'myUser'\n",
    "auth_key = 'myAuthKey'\n",
    "priv_key = 'myPrivKey'\n",
    "\n",
    "# OIDs that we want to collect\n",
    "oids = {\n",
    "    'ifInOctets11': '1.3.6.1.2.1.2.2.1.10.1',\n",
    "    'ifOutOctets11': '1.3.6.1.2.1.2.2.1.16.1',\n",
    "    'ifInDiscards11': '1.3.6.1.2.1.2.2.1.13.1',\n",
    "    'ifOutDiscards11': '1.3.6.1.2.1.2.2.1.19.1',\n",
    "    'ifInUcastPkts11': '1.3.6.1.2.1.2.2.1.11.1',\n",
    "    'ifOutUcastPkts11': '1.3.6.1.2.1.2.2.1.17.1',\n",
    "    'ifInNUcastPkts11': '1.3.6.1.2.1.2.2.1.12.1',\n",
    "    'ifOutNUcastPkts11': '1.3.6.1.2.1.2.2.1.18.1'\n",
    "}\n",
    "\n",
    "# List to store the collected data\n",
    "collected_data = []\n",
    "\n",
    "# Function to collect data from all devices\n",
    "def collect_data():\n",
    "    global collected_data\n",
    "    for device in devices:\n",
    "        device_data = {'IP': device}\n",
    "        print(f\"Collecting data from {device}\")\n",
    "        for oid_name, oid in oids.items():\n",
    "            print(f\"OID {oid_name}:\")\n",
    "            result = get_snmp_data_v3(device, oid, user, auth_key, priv_key)\n",
    "            device_data[oid_name] = result\n",
    "        collected_data.append(device_data)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Convert the collected data into a pandas DataFrame\n",
    "    df = pd.DataFrame(collected_data)\n",
    "\n",
    "    # Save the collected data to a CSV file\n",
    "    df.to_csv('collected_snmp_data.csv', index=False)\n",
    "\n",
    "    # Print the collected data\n",
    "    print(df)\n",
    "\n",
    "# Schedule the collection to run every 15 seconds\n",
    "schedule.every(15).seconds.do(collect_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize device list\n",
    "    devices = [router]  # Add more device IPs if needed\n",
    "\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b95f53",
   "metadata": {},
   "source": [
    "# Script 2 - Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccc844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the collected data into pandas DataFrame\n",
    "df = pd.DataFrame(collected_data)\n",
    "\n",
    "# Function to clean the data\n",
    "def clean_data(df):\n",
    "    # Remove duplicattes\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Converts the columns to the appropriate data type\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, invalid values become NaN\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the data\n",
    "df_cleaned = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3884a5",
   "metadata": {},
   "source": [
    "# Script 3 - Data Analysis report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674971da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "# Definir colunas dos diferentes grupos de variáveis\n",
    "interface_columns = ['ifInOctets11', 'ifOutOctets11', 'ifoutDiscards11', 'ifInUcastPkts11', 'ifInNUcastPkts11', \n",
    "                     'ifInDiscards11', 'ifOutUcastPkts11', 'ifOutNUcastPkts11']\n",
    "ip_columns = ['ipInReceives', 'ipInDelivers', 'ipOutRequests', 'ipOutDiscards', 'ipInDiscards', 'ipForwDatagrams', \n",
    "              'ipOutNoRoutes', 'ipInAddrErrors']\n",
    "icmp_columns = ['icmpInMsgs', 'icmpInDestUnreachs', 'icmpOutMsgs', 'icmpOutDestUnreachs', 'icmpInEchos', 'icmpOutEchoReps']\n",
    "tcp_columns = ['tcpOutRsts', 'tcpInSegs', 'tcpOutSegs', 'tcpPassiveOpens', 'tcpRetransSegs', 'tcpCurrEstab', 'tcpEstabResets', \n",
    "               'tcp?ActiveOpens']\n",
    "udp_columns = ['udpInDatagrams', 'udpNoPorts', 'udpInErrors', 'udpOutDatagrams']\n",
    "\n",
    "groups = {\n",
    "    'Interface': interface_columns,\n",
    "    'IP': ip_columns,\n",
    "    'ICMP': icmp_columns,\n",
    "    'TCP': tcp_columns,\n",
    "    'UDP': udp_columns\n",
    "}\n",
    "\n",
    "# Função para carregar dados\n",
    "def load_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "# Função para preparar os dados\n",
    "def prepare_data(train_data, test_data):\n",
    "    X_train = train_data.drop(columns=['class'])\n",
    "    y_train = train_data['class']\n",
    "    X_test = test_data.drop(columns=['class'])\n",
    "    y_test = test_data['class']\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Função para calcular a informação mútua\n",
    "def compute_mutual_info(X_train, y_train):\n",
    "    mi = mutual_info_classif(X_train, y_train)\n",
    "    mi_series = pd.Series(mi, index=X_train.columns).sort_values(ascending=False)\n",
    "    return mi_series\n",
    "\n",
    "# Função para plotar os scores de informação mútua\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "    plt.show()\n",
    "\n",
    "# Função para criar gráficos de linha\n",
    "def plot_line_graph(df, columns, group_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for column in columns:\n",
    "        plt.plot(df.index, df[column], label=column, alpha=0.6)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title(f'Line Plots - {group_name}')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(f'{group_name}_line_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# Função para calcular e plotar a matriz de correlação\n",
    "def plot_corr_matrix(df, columns, group_name):\n",
    "    corr_matrix = df[columns].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(f'Correlation Matrix - {group_name}')\n",
    "    plt.savefig(f'{group_name}_correlation_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# Função para executar a análise de dados\n",
    "def analyze_data(train_path, test_path):\n",
    "    train_data, test_data = load_data(train_path, test_path)\n",
    "    X_train, y_train, X_test, y_test = prepare_data(train_data, test_data)\n",
    "    \n",
    "    # Descrição geral dos dados\n",
    "    with open('data_description.txt', 'w') as f:\n",
    "        f.write(\"Descrição geral dos dados de treinamento:\\n\")\n",
    "        f.write(train_data.describe().to_string())\n",
    "    \n",
    "    # Descrição dos dados por grupo\n",
    "    for group_name, columns in groups.items():\n",
    "        with open('data_description.txt', 'a') as f:\n",
    "            f.write(f\"\\n\\nDescrição dos dados - {group_name}:\\n\")\n",
    "            f.write(train_data[columns].describe().to_string())\n",
    "        \n",
    "        # Plotar gráficos de linha\n",
    "        plot_line_graph(train_data, columns, group_name)\n",
    "        \n",
    "        # Plotar matriz de correlação\n",
    "        plot_corr_matrix(train_data, columns, group_name)\n",
    "    \n",
    "    # Calcular e plotar informação mútua\n",
    "    mi_series = compute_mutual_info(X_train, y_train)\n",
    "    plt.figure(dpi=100, figsize=(8, 5))\n",
    "    plot_mi_scores(mi_series)\n",
    "    \n",
    "    # Salvar os scores de informação mútua\n",
    "    mi_series.to_csv('mutual_information_scores.csv')\n",
    "    \n",
    "    print(\"Análise de dados completa. Relatórios e gráficos gerados.\")\n",
    "\n",
    "# Agendar a execução periódica\n",
    "def job():\n",
    "    train_path = 'path/to/train_data.csv' \n",
    "    test_path = 'path/to/test_data.csv'    \n",
    "    analyze_data(train_path, test_path)\n",
    "\n",
    "# Agendar para executar diariamente à meia-noite\n",
    "schedule.every().day.at(\"00:00\").do(job)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd59bac",
   "metadata": {},
   "source": [
    "# Script 4.1 - Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Function to load the data\n",
    "def load_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "# Function to prepare the data\n",
    "def prepare_data(train_data, test_data):\n",
    "    X_train = train_data.drop(columns=['class'])\n",
    "    y_train = train_data['class']\n",
    "    X_test = test_data.drop(columns=['class'])\n",
    "    y_test = test_data['class']\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Function to select the features\n",
    "def select_top_features(mi_path, num_features):\n",
    "    mi_series = pd.read_csv(mi_path, index_col=0, squeeze=True)\n",
    "    top_features = mi_series.head(num_features).index.tolist()\n",
    "    return top_features\n",
    "\n",
    "# Function to train and save the SVM model\n",
    "def train_and_save_model(X_train, y_train, top_features, model_path):\n",
    "    X_train_selected = X_train[top_features]\n",
    "    \n",
    "    # Data normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_selected = scaler.fit_transform(X_train_selected)\n",
    "    \n",
    "    # SVM Model Training\n",
    "    svm = SVC(class_weight='balanced', decision_function_shape='ovo', random_state=42)\n",
    "    svm.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Saving the model, the scaler, and the top features\n",
    "    model_info = {\n",
    "        'model': svm,\n",
    "        'scaler': scaler,\n",
    "        'features': top_features\n",
    "    }\n",
    "    joblib.dump(model_info, f'{model_path}.pkl')\n",
    "\n",
    "    print(\"Model trained and saved.\")\n",
    "\n",
    "# Path of the files\n",
    "train_path = 'path/to/train_data.csv'\n",
    "test_path = 'path/to/test_data.csv'\n",
    "mi_path = 'mutual_information_scores.csv'\n",
    "model_path = 'path/to/saved_model'\n",
    "\n",
    "# Load and prepare the data\n",
    "train_data, test_data = load_data(train_path, test_data)\n",
    "X_train, y_train, X_test, y_test = prepare_data(train_data, test_data)\n",
    "\n",
    "# Selecting the 15 main features\n",
    "num_features = 15  \n",
    "top_features = select_top_features(mi_path, num_features)\n",
    "\n",
    "# Training and saving the model\n",
    "train_and_save_model(X_train, y_train, top_features, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d3c4f",
   "metadata": {},
   "source": [
    "# Script 4.2 - Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from jinja2 import Template\n",
    "import schedule\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Path of the model\n",
    "model_info_path = 'model/saved_model_info.pkl'\n",
    "\n",
    "# Load the model, scaler, and features\n",
    "model_info = joblib.load(model_info_path)\n",
    "svm_model = model_info['model']\n",
    "scaler = model_info['scaler']\n",
    "top_features = model_info['features']\n",
    "\n",
    "# Function to prepare the received data\n",
    "def prepare_input(data, top_features):\n",
    "    df = pd.DataFrame(data)\n",
    "    df_selected = df[top_features]\n",
    "    df_scaled = scaler.transform(df_selected)\n",
    "    return df_scaled\n",
    "\n",
    "# Function to generate the alert report with suggestions\n",
    "def generate_alert_report(predictions, data):\n",
    "    class_counts = {cls: list(predictions).count(cls) for cls in set(predictions)}\n",
    "    abnormal_classes = {cls: count for cls, count in class_counts.items() if cls != 'normal'}\n",
    "    \n",
    "    suggestions = {\n",
    "        'tcp-syn': 'Check for potential TCP SYN flood attacks and review firewall rules.',\n",
    "        'udp-flood': 'Inspect for UDP flood attacks and consider rate limiting.',\n",
    "        'icmp-echo': 'Investigate ICMP echo requests for potential ping floods.',\n",
    "        'httpFlood': 'Analyze HTTP traffic for potential HTTP flood attacks.'\n",
    "    }\n",
    "    \n",
    "    abnormal_suggestions = {cls: suggestions.get(cls, 'No specific suggestion available.') for cls in abnormal_classes}\n",
    "\n",
    "    template_str = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Alert Report</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; }\n",
    "            h1 { text-align: center; }\n",
    "            .section { margin-bottom: 40px; }\n",
    "            .section h2 { border-bottom: 1px solid #000; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Alert Report</h1>\n",
    "        <div class=\"section\">\n",
    "            <h2>Results of the classification model:</h2>\n",
    "            <ul>\n",
    "                {% for class_name, count in class_counts.items() %}\n",
    "                    <li>{{ class_name }} class = {{ count }} entries</li>\n",
    "                {% endfor %}\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h2>Abnormal Classes Detected:</h2>\n",
    "            {% if abnormal_classes %}\n",
    "                <ul>\n",
    "                {% for class_name, count in abnormal_classes.items() %}\n",
    "                    <li>{{ class_name }}: {{ count }} entries</li>\n",
    "                {% endfor %}\n",
    "                </ul>\n",
    "            {% else %}\n",
    "                <p>No abnormal classes detected.</p>\n",
    "            {% endif %}\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h2>Suggestions for Resolution:</h2>\n",
    "            {% if abnormal_suggestions %}\n",
    "                <ul>\n",
    "                {% for class_name, suggestion in abnormal_suggestions.items() %}\n",
    "                    <li>{{ class_name }}: {{ suggestion }}</li>\n",
    "                {% endfor %}\n",
    "                </ul>\n",
    "            {% else %}\n",
    "                <p>No suggestions available.</p>\n",
    "            {% endif %}\n",
    "        </div>\n",
    "        <div class=\"section\">\n",
    "            <h2>Received data</h2>\n",
    "            <pre>{{ data }}</pre>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    template = Template(template_str)\n",
    "    html_out = template.render(class_counts=class_counts, abnormal_classes=abnormal_classes, \n",
    "                               abnormal_suggestions=abnormal_suggestions, data=data.to_string())\n",
    "    \n",
    "    report_path = 'alert_report.html'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(html_out)\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "# Function to make predictions and generate alert reports if necessary\n",
    "def predict_and_alert():\n",
    "    # Get the latest CSV files from the last 5 minutes\n",
    "    data_path = 'data'\n",
    "    current_time = datetime.now()\n",
    "    five_minutes_ago = current_time - timedelta(minutes=5)\n",
    "    \n",
    "    list_of_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.startswith('collected_snmp_data_') and \n",
    "                     f.endswith('.csv')]\n",
    "    recent_files = [f for f in list_of_files if datetime.fromtimestamp(os.path.getctime(f)) > five_minutes_ago]\n",
    "    \n",
    "    # Load and concatenate the recent files\n",
    "    if recent_files:\n",
    "        df_list = [pd.read_csv(f) for f in recent_files]\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        # Make predictions and generate an alert report if necessary\n",
    "        input_data = prepare_input(df, top_features)\n",
    "        predictions = svm_model.predict(input_data)\n",
    "        \n",
    "        report_path = generate_alert_report(predictions, df)\n",
    "        print(f\"Report generated: {report_path}\")\n",
    "    else:\n",
    "        print(\"No recent data files found.\")\n",
    "\n",
    "# Endpoint for prediction via API\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    input_data = prepare_input(data, top_features)\n",
    "    \n",
    "    predictions = svm_model.predict(input_data)\n",
    "    \n",
    "    report_path = generate_alert_report(predictions, pd.DataFrame(data))\n",
    "    return jsonify({\"message\": \"Prediction completed.\", \"report\": report_path}), 200\n",
    "\n",
    "# Schedule the prediction to run every 5 minutes\n",
    "schedule.every(5).minutes.do(predict_and_alert)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask server\n",
    "    app.run(debug=True)\n",
    "    \n",
    "    # Schedule the prediction\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
